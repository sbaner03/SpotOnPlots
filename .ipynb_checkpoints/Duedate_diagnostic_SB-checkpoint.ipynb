{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import sframe as gl\n",
    "import sframe.aggregate as agg\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from itertools import chain, islice, repeat, imap,izip,starmap,product\n",
    "import random\n",
    "import timeit as t\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "from pandas import ExcelWriter\n",
    "import smtplib\n",
    "from email import Encoders\n",
    "from email.MIMEBase import MIMEBase\n",
    "from email.MIMEMultipart import MIMEMultipart\n",
    "from email.Utils import formatdate\n",
    "from email.mime.text import MIMEText\n",
    "from email import encoders\n",
    "import os\n",
    "import ftplib\n",
    "import traceback\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "#conthc = gl.SFrame('conthcdf.csv')\n",
    "conthc = gl.SFrame(r'http://spoton.co.in/downloads/IEP_DUEDATE_DATA_REPORT/IEP_DUEDATE_DATA_REPORT.csv')\n",
    "#### conthc = gl.SFrame(r'D:\\Python\\Scripts and Files\\Path and Graph Files\\IEP_DUEDATE_DATA_REPORT.csv')\n",
    "pathsf = gl.SFrame.read_csv(r'/home/stl/Python/Files_python/Due_date_diagnostic_inputs/final_pathv3.csv')\n",
    "lhscheduledf = gl.SFrame.read_csv(r'/home/stl/Python/Files_python/Due_date_diagnostic_inputs/Timegraph.csv')\n",
    "vbdf = pd.read_csv(r'/home/stl/Python/Files_python/Due_date_diagnostic_inputs/Hublist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "conthc = conthc.rename({'\\xef\\xbb\\xbfDOCKNO': 'DOCKNO'})\n",
    "fullconthc = copy.deepcopy(conthc) #to be used later for % movement calculation\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "#conthc.column_names()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "print len(conthc)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#conthc = conthc.filter_by(['NULL'],'arrv_dt', exclude=True).filter_by(['NULL'],'dept_dt', exclude=True)\n",
    "conthc = conthc.filter_by([''],'arrv_dt', exclude=True).filter_by([''],'dept_dt', exclude=True)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "print len(conthc)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "timeformatstring = '%d/%m/%Y %H:%M'\n",
    "timeformatstring1 = '%d-%m-%Y %H:%M'\n",
    "timeformatstring2 = '%Y-%m-%d %H:%M:%S'\n",
    "#dayzero = datetime.strptime('1/1/2015 00:00', timeformatstring)\n",
    "#dayminu = datetime.strptime('1/1/2014 00:00', timeformatstring)\n",
    "dayzero = datetime.strptime('2015-1-1 00:00:00', timeformatstring2)\n",
    "dayminu = datetime.strptime('2014-1-1 00:00:00', timeformatstring2)\n",
    "dayzero, dayminu\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "#conthc['dept_dt'][0], conthc['arrv_dt'][0], conthc['DUE_DATE'][0]\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "conthc['dept_dt']= conthc.apply(lambda x: datetime.strptime(x['dept_dt'],timeformatstring2))\n",
    "conthc['arrv_dt']= conthc.apply(lambda x: datetime.strptime(x['arrv_dt'],timeformatstring2))\n",
    "conthc['DUE_DATE']= conthc.apply(lambda x: datetime.strptime(x['DUE_DATE'],timeformatstring2))\n",
    "\n",
    "\n",
    "## Edited to exclude ODA extra days from DUEDATE\n",
    "conthc = conthc.fillna('ODAExtraTransitDay', 0)\n",
    "conthc['DUE_DATE'] = conthc.apply(lambda x: x['DUE_DATE']-timedelta (days=int(x['ODAExtraTransitDay'])))\n",
    "## Edited to exclude ODA extra days from DUEDATE\n",
    "\n",
    "# In[13]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getatdestn(currloc, destination):\n",
    "    if currloc == destination:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "conthc['at_destn'] = conthc.apply(lambda x: getatdestn(x['DOC_CURLOC'],x['DESTN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "\n",
    "lhscheduledfgrp = lhscheduledf.groupby(['Origin','Destination','Type'],{'Departure Time':agg.CONCAT('Departure Time'),'Total leg TT': agg.CONCAT('Total leg TT')})\n",
    "lhscheduledf_dict = {}\n",
    "for org, dest, deptime, transithrs,vbtype in izip(lhscheduledfgrp['Origin'], lhscheduledfgrp['Destination'],\n",
    "                                     lhscheduledfgrp['Departure Time'], lhscheduledfgrp['Total leg TT'], lhscheduledfgrp['Type']):\n",
    "    lhscheduledf_dict[(org, dest)] = [deptime,transithrs,vbtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "\n",
    "path_dict = {}\n",
    "for org, dest, paths in izip(pathsf['Origin'], pathsf['Destination'],pathsf['pathlist']):\n",
    "    path_dict[(org, dest)] = paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[17]:\n",
    "\n",
    "def getconpath(origin,location,destination):\n",
    "    #print origin, location, destination\n",
    "    try:\n",
    "        conpathall1 = path_dict.get((origin,destination))\n",
    "        if conpathall1 is None:\n",
    "            conpathall = None\n",
    "        else:\n",
    "            conpathall = [i for i in conpathall1 if location in i]\n",
    "        auxpathall = path_dict.get((location,destination))\n",
    "        if conpathall is None:\n",
    "            conpathall = []\n",
    "        if auxpathall is None:\n",
    "            auxpathall = []\n",
    "        if (len(conpathall)==0) and (len(auxpathall)==0):\n",
    "            return ['error']\n",
    "        elif (len(conpathall)==0):\n",
    "            lenlist = [len(i) for i in auxpathall]\n",
    "            return auxpathall[lenlist.index(min(lenlist))]\n",
    "        else:\n",
    "            lenlist = [len(i) for i in conpathall]\n",
    "            minconpath = conpathall[lenlist.index(min(lenlist))]\n",
    "            return minconpath[minconpath.index(location):]\n",
    "    except:\n",
    "        ['checkerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "\n",
    "def getdeparturetime(org, dest, currtime):\n",
    "    try:\n",
    "        currtimedt = str(currtime).split(' ')[0]+' 00:00:00'\n",
    "        currtimedt = datetime.strptime(currtimedt,'%Y-%m-%d %H:%M:%S')\n",
    "        deptime_total = lhscheduledf_dict.get((org, dest))\n",
    "        if deptime_total is None:\n",
    "            return dayzero, dayzero\n",
    "        deptime, transithrs, vbtype = deptime_total\n",
    "        if vbtype == 'Normal':\n",
    "            timerellist = []\n",
    "            for i in range(0,len(deptime)):\n",
    "                deptime1 = timedelta(hours=deptime[i])+(currtimedt)\n",
    "                deptime2 = timedelta(hours=(deptime[i]+24.0))+(currtimedt)\n",
    "                arrtime1 = deptime1 + timedelta(hours=transithrs[i])\n",
    "                arrtime2 = deptime2 + timedelta(hours=transithrs[i])\n",
    "                if deptime1<=currtime:\n",
    "                    timerellist.append((deptime2,arrtime2))\n",
    "                else:\n",
    "                    timerellist.append((deptime1,arrtime1))\n",
    "            \n",
    "            sorted_by_first = sorted(timerellist, key=lambda tup: tup[0], reverse=False)\n",
    "            return sorted_by_first[0][0],sorted_by_first[0][1]\n",
    "        elif vbtype == 'VB':\n",
    "            deptime = timedelta(hours=3.0)+(currtime)\n",
    "            return deptime,deptime\n",
    "    except:\n",
    "        return dayzero,dayzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[19]:\n",
    "\n",
    "def getidealtimes(con,conpath,pickuptime):\n",
    "    ### pickuptime is current time\n",
    "    try:\n",
    "        idealtimelist = []\n",
    "        currtime = pickuptime\n",
    "        idealtimelist.append(pickuptime)\n",
    "        for i in range (1,len(conpath)):\n",
    "            org = conpath[i-1]\n",
    "            dest = conpath[i]\n",
    "            departuretime,arrivaltime = getdeparturetime(org,dest,currtime)\n",
    "            if (departuretime==dayzero)or(arrivaltime==dayzero):\n",
    "                idealtimelist=[dayzero]\n",
    "                break    \n",
    "            else:\n",
    "                if (dest == conpath[-1]) and (departuretime.weekday()==arrivaltime.weekday()==6) and (arrivaltime.hour<=12):\n",
    "                    departuretime = departuretime+timedelta(hours=24)\n",
    "                    arrivaltime = arrivaltime+timedelta(hours=24)\n",
    "                idealtimelist.append(departuretime)\n",
    "                idealtimelist.append(arrivaltime)\n",
    "                currtime = arrivaltime\n",
    "\n",
    "        return idealtimelist\n",
    "    except:\n",
    "        return [dayzero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[20]:\n",
    "\n",
    "def getdepartedvehicletime(thcorigin, thcdest,actualdeptime):\n",
    "    try:\n",
    "        deptime_total = lhscheduledf_dict.get((thcorigin, thcdest))\n",
    "        if deptime_total is None:\n",
    "            return dayzero, dayzero\n",
    "        deptime, transithrs, vbtype = deptime_total\n",
    "        if vbtype == 'Normal':\n",
    "            timerellist = []\n",
    "            for i in range(0,len(deptime)):\n",
    "                deptime1 = actualdeptime\n",
    "                arrtime1 = actualdeptime + timedelta(hours=transithrs[i])\n",
    "                timerellist.append((deptime1,arrtime1))\n",
    "            \n",
    "            sorted_by_first = sorted(timerellist, key=lambda tup: tup[0], reverse=False)\n",
    "            return sorted_by_first[0][0],sorted_by_first[0][1]\n",
    "        elif vbtype == 'VB':\n",
    "            arrtime = timedelta(hours=1.0)+(actualdeptime)\n",
    "            return arrtime,arrtime\n",
    "    except:\n",
    "        return dayzero,dayzero\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[21]:\n",
    "\n",
    "def getidealtimes_source(con,conpath,thcorigin,thcdest,deptime):\n",
    "    ### pickuptime is current time\n",
    "    try:\n",
    "        idealtimelist = []\n",
    "        idealtimelist.append(deptime)\n",
    "        currtime = getdepartedvehicletime(thcorigin, thcdest,deptime)[1]\n",
    "        idealtimelist.append(currtime)\n",
    "        for i in range (2,len(conpath)):\n",
    "            org = conpath[i-1]\n",
    "            dest = conpath[i]\n",
    "            departuretime,arrivaltime = getdeparturetime(org,dest,currtime)\n",
    "            if (departuretime==dayzero)or(arrivaltime==dayzero):\n",
    "                idealtimelist=[dayzero]\n",
    "                break    \n",
    "            else:\n",
    "                if (dest == conpath[-1]) and (departuretime.weekday()==arrivaltime.weekday()==6) and (arrivaltime.hour<=12):\n",
    "                    departuretime = departuretime+timedelta(hours=24)\n",
    "                    arrivaltime = arrivaltime+timedelta(hours=24)\n",
    "                idealtimelist.append(departuretime)\n",
    "                idealtimelist.append(arrivaltime)\n",
    "                currtime = arrivaltime\n",
    "\n",
    "        return idealtimelist\n",
    "    except:\n",
    "        return [dayzero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[22]:\n",
    "\n",
    "conthc['conpath_source'] = conthc.apply(lambda x: getconpath(x['ORGIN'],x['sourcehub'],x['DESTN']))\n",
    "conthc['conpath_toloc'] = conthc.apply(lambda x: getconpath(x['ORGIN'],x['TOHUB_BR'],x['DESTN']))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "conthc['idealtimelist_source'] = conthc.apply(lambda x: getidealtimes_source(x['DOCKNO'],x['conpath_source'],x['sourcehub'],x['TOHUB_BR'],x['dept_dt']))\n",
    "conthc['idealtimelist_toloc'] = conthc.apply(lambda x: getidealtimes(x['DOCKNO'],x['conpath_toloc'],x['arrv_dt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conthc.save('conthc_check.csv')\n",
    "# In[24]:\n",
    "\n",
    "dayzero_conlist_s = list(conthc[conthc['idealtimelist_source']==[dayzero]]['DOCKNO'])\n",
    "dayzero_conlist_t = list(conthc[conthc['idealtimelist_toloc']==[dayzero]]['DOCKNO'])\n",
    "print dayzero_conlist_s, type(dayzero_conlist_s)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "conthc = conthc.filter_by(dayzero_conlist_s, 'DOCKNO', exclude=True)\n",
    "conthc = conthc.filter_by(dayzero_conlist_t, 'DOCKNO', exclude=True)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "conthc = conthc[conthc['idealtimelist_source']!=[dayzero]]\n",
    "conthc = conthc[conthc['idealtimelist_toloc']!=[dayzero]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[27]:\n",
    "\n",
    "def geteta(x):\n",
    "    try:\n",
    "        return x[-1]\n",
    "    except:\n",
    "        return dayminu\n",
    "\n",
    "def getreach(dockno, eta,duedt):\n",
    "    ##print dockno, eta, duedt\n",
    "    duedtadj = duedt+timedelta(hours=14)\n",
    "    ##print duedtadj\n",
    "    \n",
    "    if eta<=duedtadj:\n",
    "        ##print 'inside if', 0, dockno, eta, duedt\n",
    "        return 0\n",
    "    else:\n",
    "        if eta.hour <= 13:\n",
    "            eta_delivery = eta.date()\n",
    "        else:\n",
    "            eta_delivery = (eta+timedelta(days=1)).date()\n",
    "        td = (eta_delivery - duedt.date()).days\n",
    "        ##print 'inside else', td, dockno, eta, duedt\n",
    "        return td\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[28]:\n",
    "\n",
    "conthc['eta_source'] = conthc.apply(lambda x: geteta(x['idealtimelist_source']))\n",
    "conthc['eta_toloc'] = conthc.apply(lambda x: geteta(x['idealtimelist_toloc']))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "conthc = conthc[conthc['eta_source']!=dayminu]\n",
    "conthc = conthc[conthc['eta_toloc']!=dayminu]\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "#help needed: some cons like 511201627 is not appearing in the below print statements. The arrival at dest SC for this con is\n",
    "#at 1416 hrs of its due date. But its 'reach_source' is 0. Not able to debug because of it's absence\n",
    "conthc['reach_source'] = conthc.apply(lambda x: getreach(x['DOCKNO'],x['eta_source'],x['DUE_DATE']))\n",
    "conthc['reach_toloc'] = conthc.apply(lambda x: getreach(x['DOCKNO'],x['eta_toloc'],x['DUE_DATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[31]:\n",
    "\n",
    "len(conthc)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "conthcfailed = conthc[conthc['REACH']=='NO']\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "conthcfailedts = conthcfailed.groupby(['DOCKNO','at_destn'],{'org': agg.CONCAT('sourcehub'),'dest': agg.CONCAT('TOHUB_BR'),'reach_source': agg.CONCAT('reach_source'),'reach_toloc': agg.CONCAT('reach_toloc'), 'conpath_source': agg.CONCAT('conpath_source'), 'conpath_toloc': agg.CONCAT('conpath_toloc')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[34]:\n",
    "\n",
    "conthcfailed_dict = {}\n",
    "for con,org,reach_source,dest,reach_toloc,atdest,conpath_toloc in izip(conthcfailedts['DOCKNO'],conthcfailedts['org'],conthcfailedts['reach_source'],conthcfailedts['dest'],conthcfailedts['reach_toloc'], conthcfailedts['at_destn'],conthcfailedts['conpath_toloc']):\n",
    "    conthcfailed_dict.update({con: (org,reach_source,dest,reach_toloc, atdest,conpath_toloc)})\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "reason_dict = {}\n",
    "for con in conthcfailed_dict:\n",
    "    \n",
    "    vals = conthcfailed_dict.get(con)\n",
    "    sourcelist = vals[0]\n",
    "    toloclist = vals[2]\n",
    "    source_delays = vals[1]\n",
    "    toloc_delays = vals[3]\n",
    "    atdest = vals[4]\n",
    "    conpath_toloc = vals[5]\n",
    "    source_hike_node = -1\n",
    "    toloc_hike_node = -1\n",
    "    if len(sourcelist) == 1:\n",
    "        if source_delays[0] > 0:\n",
    "            source_hike_node = 0\n",
    "    else: \n",
    "        for i in range (0, len(sourcelist)-1):\n",
    "            if source_delays[0] > 0:\n",
    "                source_hike_node = 0\n",
    "                break\n",
    "            #print con, len(sourcelist), source_delays, 'i is', i\n",
    "            if source_delays[i+1]>source_delays[i]:\n",
    "                source_hike_node = i+1\n",
    "                break \n",
    "                \n",
    "    if len(toloclist) == 1:\n",
    "        if toloc_delays[0] > 0:\n",
    "            toloc_hike_node = 0\n",
    "    else:  \n",
    "        for i in range (0, len(toloclist)-1):\n",
    "            if toloc_delays[0] > 0:\n",
    "                toloc_hike_node = 0\n",
    "                break\n",
    "            if toloc_delays[i+1] > toloc_delays[i]:\n",
    "                toloc_hike_node = i+1\n",
    "                break\n",
    "    #print con, source_hike_node, toloc_hike_node  \n",
    "    #print source, toloc\n",
    "    \n",
    "    if toloc_delays[len(toloclist)-1]== 0: #to exclude which has reached the dest SC on time\n",
    "        if atdest == 'Yes':\n",
    "            reason_dict.update({con: ('Reached','Lastvalue0',source_hike_node,toloc_hike_node)})\n",
    "        else:\n",
    "            try:\n",
    "                nxtloctn = conpath_toloc[len(toloclist)-1][1]\n",
    "            except:\n",
    "                nxtloctn = 'error'\n",
    "            reason_dict.update({con: ('Location',str(toloclist[len(toloclist)-1])+str('-')+str(nxtloctn),source_hike_node,toloc_hike_node)})\n",
    "            #Loc-ND\n",
    "    elif source_hike_node == -1:\n",
    "        toloc_hike_node = len(toloclist)\n",
    "        source_hike_node = len(sourcelist)\n",
    "        reason_dict.update({con: ('LH',str(sourcelist[-1])+str('-')+str(toloclist[-1]),source_hike_node,toloc_hike_node)})\n",
    "        #LastLeg\n",
    "        #print 'nothing'\n",
    "    else:\n",
    "        if source_hike_node == toloc_hike_node:\n",
    "            #print 'location', source[source_hike_node]\n",
    "            reason_dict.update({con: ('Location',str(sourcelist[source_hike_node])+str('-')+str(toloclist[toloc_hike_node]),source_hike_node,toloc_hike_node)})\n",
    "        else:\n",
    "            #print 'LH>', source[toloc_hike_node-1],toloc[toloc_hike_node]\n",
    "            reason_dict.update({con: ('LH',str(sourcelist[toloc_hike_node])+str('-')+str(toloclist[toloc_hike_node]),source_hike_node,toloc_hike_node)})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[36]:\n",
    "\n",
    "reasonsf = gl.SFrame()\n",
    "conlist = []\n",
    "categorylist = []\n",
    "specificlist = []\n",
    "sourcehnlist = []\n",
    "tolochnlist = []\n",
    "xy = ['DOCKNO', 'Reason', 'Specific_Pt', 'S_hikenode', 'T_hikenode']\n",
    "for con1 in reason_dict:\n",
    "    vals1 = reason_dict.get(con1)\n",
    "    category = vals1[0]\n",
    "    specific_pt = vals1[1]\n",
    "    s_hn = vals1[2]\n",
    "    tl_hn = vals1[3]\n",
    "    conlist.append(con1)\n",
    "    categorylist.append(category)\n",
    "    specificlist.append(specific_pt)\n",
    "    sourcehnlist.append(s_hn)\n",
    "    tolochnlist.append(tl_hn)\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "conarray = gl.SArray(conlist)\n",
    "categoryarray = gl.SArray(categorylist)\n",
    "specificarray = gl.SArray(specificlist)\n",
    "source_hnarray = gl.SArray(sourcehnlist)\n",
    "toloc_hnarray = gl.SArray(tolochnlist)\n",
    "\n",
    "reasonsf.add_column(conarray, name='DOCKNO')\n",
    "reasonsf.add_column(categoryarray, name='Reason')\n",
    "reasonsf.add_column(specificarray, name='specific_pt')\n",
    "reasonsf.add_column(source_hnarray, name='S_hikenode')\n",
    "reasonsf.add_column(toloc_hnarray, name='T_hikenode')\n",
    "#reasonsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[38]:\n",
    "\n",
    "finalsf = conthcfailedts.join(reasonsf, on='DOCKNO', how='left')\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "def finderror(sourcepath, tolocpath):\n",
    "    if 'error' in [j for i in sourcepath for j in i]:\n",
    "        return 'found error'\n",
    "    elif 'error' in [j for i in tolocpath for j in i]:\n",
    "        return 'found error'\n",
    "    else: \n",
    "        return 'clean'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[40]:\n",
    "\n",
    "finalsf['check_error'] = finalsf.apply(lambda x: finderror(x['conpath_source'],x['conpath_toloc']))\n",
    "#finaldf = finaldf[finaldf['conpath_source'].str.contains('error')]\n",
    "#df3= paths[(paths['Destination']== destinationtcr) & (paths['Path1'].str.contains(currloc))]\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "#finalsf.save(r'finalsf_full.csv')\n",
    "finalsf = finalsf[finalsf['check_error']=='clean']   \n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "finalsf = finalsf[finalsf['Reason']!='Reached'] #eliminate where getting as reached  To remove comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[43]:\n",
    "\n",
    "datetoday = datetime.today()\n",
    "datefilter = datetoday-timedelta(hours=24)\n",
    "datefilter=datefilter.date()\n",
    "datefilter\n",
    "\n",
    "\n",
    "basedata = conthcfailed.join(finalsf, on='DOCKNO',how='inner')\n",
    "#basedata.save('basedata.csv')\n",
    "basedatadf = basedata.to_dataframe()\n",
    "\n",
    "basedatadf.to_csv(r'/home/stl/Python/Data/Duedate_diagnostic/Basedata/Duedate_diagnostic_data_'+str(datefilter)+'.csv')\n",
    "\n",
    "basedatadf.to_csv(r'/home/stl/Python/Data/Duedate_diagnostic/Duedate_diagnostic_data.csv')  \n",
    "#basedatadf.to_csv(r'D:\\Data\\Duedate_diagnostic\\Duedate_diagnostic_data_test.csv')  # To remove line\n",
    "oppath_data = r'/home/stl/Python/Data/Duedate_diagnostic/Duedate_diagnostic_data.csv'\n",
    "\n",
    "#fosdhfsodhfodsh  ##To remove comment\n",
    "## Edit for Oneline LH THC data for JM\n",
    "basedatadf['Concate'] = basedatadf.apply(lambda x:(x['sourcehub']+str('-')+x['TOHUB_BR']),axis=1)\n",
    "uniquedf_lh = basedatadf[basedatadf['Reason']=='LH'][['DOCKNO','specific_pt']]\n",
    "uniquedf_lh = uniquedf_lh.drop_duplicates(subset='DOCKNO')\n",
    "\n",
    "lh_fail_1liner = pd.merge(uniquedf_lh,basedatadf,left_on=['DOCKNO','specific_pt'],right_on=['DOCKNO','Concate'],how='inner')\n",
    "lh_fail_1liner = lh_fail_1liner.drop_duplicates(subset='DOCKNO')\n",
    "\n",
    "lh_fail_1liner.to_csv(r'/home/stl/Python/Data/Duedate_diagnostic/Basedata_LH/Duedate_diagnostic_LH_oneliner_'+str(datefilter)+'.csv')\n",
    "\n",
    "lh_fail_1liner.to_csv(r'/home/stl/Python/Data/Duedate_diagnostic/Duedate_diagnostic_LH_oneliner.csv')\n",
    "oppath_lhfail_1liner = r'/home/stl/Python/Data/Duedate_diagnostic/Duedate_diagnostic_LH_oneliner.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('Logging in...')\n",
    "ftp = ftplib.FTP()\n",
    "#ftp.connect('119.226.230.94')\n",
    "ftp.connect('10.109.230.50')\n",
    "print (ftp.getwelcome())\n",
    "try:\n",
    "    try:\n",
    "        ftp.login('IEPROJECTUSER', 'spotStar@123')\n",
    "        ftp.cwd('ETA')\n",
    "        # move to the desired upload directory\n",
    "        print (\"Currently in:\", ftp.pwd())\n",
    "        print ('Uploading...')\n",
    "        fullname = oppath_lhfail_1liner\n",
    "        name = os.path.split(fullname)[1]\n",
    "        f = open(fullname, \"rb\")\n",
    "        ftp.storbinary('STOR ' + name, f)\n",
    "        f.close()\n",
    "        print (\"OK\"  )\n",
    "        print (\"Files:\")\n",
    "        print (ftp.retrlines('LIST'))\n",
    "    finally:\n",
    "        print (\"Quitting...\")\n",
    "        ftp.quit()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "## Edit for Oneline LH THC data for JM\n",
    "\n",
    "print ('Logging in...')\n",
    "ftp = ftplib.FTP()\n",
    "#ftp.connect('119.226.230.94')\n",
    "ftp.connect('10.109.230.50')\n",
    "print (ftp.getwelcome())\n",
    "try:\n",
    "    try:\n",
    "        ftp.login('IEPROJECTUSER', 'spotStar@123')\n",
    "        ftp.cwd('ETA')\n",
    "        # move to the desired upload directory\n",
    "        print (\"Currently in:\", ftp.pwd())\n",
    "        print ('Uploading...')\n",
    "        fullname = oppath_data\n",
    "        name = os.path.split(fullname)[1]\n",
    "        f = open(fullname, \"rb\")\n",
    "        ftp.storbinary('STOR ' + name, f)\n",
    "        f.close()\n",
    "        print (\"OK\"  )\n",
    "        print (\"Files:\")\n",
    "        print (ftp.retrlines('LIST'))\n",
    "    finally:\n",
    "        print (\"Quitting...\")\n",
    "        ftp.quit()\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "\n",
    "#FTP Upload ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[44]:\n",
    "\n",
    "finalsfgrp1 = finalsf.groupby(['specific_pt','Reason'],{'DOCKNO': agg.COUNT('DOCKNO')})\n",
    "#gettin the full con grupby\n",
    "fullcongpby = fullconthc.groupby(['sourcehub','TOHUB_BR'], {'TOTAL': agg.COUNT_DISTINCT('DOCKNO')})\n",
    "fullcongpby['specific_pt']= fullcongpby.apply(lambda x: x['sourcehub']+str('-')+x['TOHUB_BR'])\n",
    "fullcongpby1 = fullcongpby[['specific_pt',  'TOTAL' ]]\n",
    "fullcongpby1.save(r'/home/stl/Python/Data/Duedate_diagnostic/Summary/CheckDuedate.csv')\n",
    "#joining this with the result of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalsfgrp = finalsfgrp1.join(fullcongpby1, on = 'specific_pt', how='left')\n",
    "finalsfgrp = finalsfgrp.fillna('TOTAL', 0)\n",
    "def adjust1(doc, tot):\n",
    "    if tot < doc:\n",
    "        return doc\n",
    "    else:\n",
    "        return tot\n",
    "\n",
    "#changing total as cons wouldn't move from a location and will appear as failure but will not appear in total\n",
    "finalsfgrp['TOTAL'] = finalsfgrp.apply(lambda x: adjust1(x['DOCKNO'],x['TOTAL']))        \n",
    "finalsfgrp.save(r'/home/stl/Python/Data/Duedate_diagnostic/Summary/CheckDuedate2.csv')\n",
    "finalsfgrp['%'] = finalsfgrp.apply(lambda x: pd.np.round(x['DOCKNO']*100/x['TOTAL']*1.0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[45]:\n",
    "\n",
    "#finalsfgrp = finalsfgrp[finalsfgrp['Reason']!='Reached'] #eliminate where getting as reached\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "###finalsfgrp = finalsfgrp.sort('DOCKNO', ascending=False)\n",
    "\n",
    "\n",
    "## For sorting based\n",
    "finalsfgrp = finalsfgrp.sort(['DOCKNO','%'], ascending=False)\n",
    "\n",
    "\n",
    "# In[47]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finalsfgrp.save('finalsfgrp.csv')\n",
    "finalsfgrp_df = finalsfgrp.to_dataframe()\n",
    "finalsfgrp_df['Timestamp'] = finalsfgrp_df.apply(lambda x:datefilter,axis=1)\n",
    "\n",
    "finalsfgrp_df.to_csv(r'/home/stl/Python/Data/Duedate_diagnostic/Summary/Duedate_diagnostic_summary_'+str(datefilter)+'.csv')\n",
    "\n",
    "finalsfgrp_df.to_csv(r'/home/stl/Python/Data/Duedate_diagnostic/Duedate_diagnostic_summary.csv')\n",
    "oppath_summary = r'/home/stl/Python/Data/Duedate_diagnostic/Duedate_diagnostic_summary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For publishing email body variables\n",
    "finalsfgrp_df_lh = finalsfgrp_df[finalsfgrp_df['Reason']=='LH']\n",
    "lhfailcons = finalsfgrp_df_lh['DOCKNO'].sum()\n",
    "print 'lhfailcons',lhfailcons\n",
    "finalsfgrp_df_loc = finalsfgrp_df[finalsfgrp_df['Reason']=='Location']\n",
    "locfailcons = finalsfgrp_df_loc['DOCKNO'].sum()\n",
    "\n",
    "totalfailcons = finalsfgrp_df['DOCKNO'].sum()\n",
    "\n",
    "lhfailperc = pd.np.round((lhfailcons*100.0)/totalfailcons,0)\n",
    "locfailperc = pd.np.round((locfailcons*100.0)/totalfailcons,0)\n",
    "## For publishing email body variables\n",
    "\n",
    "\n",
    "finalsfgrp_df_mail = finalsfgrp_df.head(50)\n",
    "#finalsfgrp_df_mail = finalsfgrp_df\n",
    "# In[ ]:\n",
    "\n",
    "finalsfgrp_df_mail = finalsfgrp_df_mail.to_string(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "filePath = oppath_summary\n",
    "def sendEmail(#TO = [\"sqtf@spoton.co.in\",\"sq_spot@spoton.co.in\",\"rom_spot@spoton.co.in\",\"dom_spot@spoton.co.in\"],\n",
    "             #TO = [\"vishwas.j@spoton.co.in\"],\n",
    "             #CC =  [\"vishwas.j@spoton.co.in\"],\n",
    "             #CC =  [\"supratim@iepfunds.com\",\"rajeesh.vr@spoton.co.in\"],\n",
    "             #BCC =  [\"vishwas.j@spoton.co.in\"],\n",
    "            #FROM=\"mis.ho@spoton.co.in\"):\n",
    "            FROM=\"reports.ie@spoton.co.in\"):\n",
    "    HOST = \"smtp.spoton.co.in\"\n",
    "    #HOST = \"smpt.spoton.co.in\"\n",
    "#smtplib.SMTP('smtp.spoton.co.in', 25)\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg[\"From\"] = FROM\n",
    "    msg[\"To\"] = \",\".join(TO)\n",
    "    msg[\"CC\"] = \",\".join(CC)\n",
    "    msg[\"BCC\"] = \",\".join(BCC)\n",
    "    #msg[\"Subject\"] = \"DEPOTWISE OPS Bucket Stock  - \"+ str(datefilter)\n",
    "    msg[\"Subject\"] = \"Duedate Diagnostic Report L - \"+ str(datefilter)\n",
    "    body_text = \"\"\"\n",
    "    Dear All,\n",
    "    \n",
    "    PFB the Duedate Diagnostic Report of Duedate \"\"\" + str(datefilter) +\"\"\" which fail to reach the Destination SC. \n",
    "    \n",
    "    Number of LH failed cons = \"\"\"+str(lhfailcons)+\"\"\"\n",
    "    Number of Location failed cons = \"\"\"+str(locfailcons)+\"\"\"\n",
    "    LH Failed cons % = \"\"\"+str(lhfailperc)+\"\"\" %\n",
    "    Location Failed cons % = \"\"\"+str(locfailperc)+\"\"\" %\n",
    "       \n",
    "     Note:\n",
    "    'LH' means the reason is Linehaul. This means , the con was expected to reach while departing from LH Origin. However when received at LH Destination, the con failed ETA \n",
    "    The 'specific_point' attributes the problem to the specific lane.\n",
    "    \n",
    "    'Location' means the reason is location. This means the con was expected to reach ETA when the location received a con. However the con failed ETA at the time of departure\n",
    "    The 'specific_point' attributes the problem to the location-sector.\n",
    "    \n",
    "    Total no of cons are mentioned under 'DOCKNO'\n",
    "            \n",
    "    \n",
    "\"\"\"+str(finalsfgrp_df_mail)+\"\"\"\n",
    "\n",
    "\n",
    "    For conwise data, please download from the link below\n",
    "        \n",
    "    http://spoton.co.in/downloads/IEProjects/ETA/Duedate_diagnostic_data.csv\n",
    "    \n",
    "    For LH Failed condata, pleae use the link below. \n",
    "    \n",
    "    http://spoton.co.in/downloads/IEProjects/ETA/Duedate_diagnostic_LH_oneliner.csv\n",
    "    \"\"\"\n",
    "    if body_text:\n",
    "        msg.attach( MIMEText(body_text) )\n",
    "    part = MIMEBase('application', \"octet-stream\")\n",
    "    part.set_payload( open(filePath,\"rb\").read() )\n",
    "    Encoders.encode_base64(part)\n",
    "    part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % os.path.basename(filePath))\n",
    "    msg.attach(part)\n",
    "    server=smtplib.SMTP('smtp.spoton.co.in', 587)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    server.ehlo()\n",
    "    #server.login(\"mis.ho@spoton.co.in\", \"Spoton@123\")\n",
    "    server.login(\"reports.ie@spoton.co.in\", \"Reports@sep2016\")\n",
    "\n",
    "    try:\n",
    "        failed = server.sendmail(FROM, TO+CC+BCC, msg.as_string())\n",
    "        server.close()\n",
    "    except Exception, e:\n",
    "        errorMsg = \"Unable to send email. Error: %s\" % str(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sendEmail()\n",
    "#print('Email sent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
